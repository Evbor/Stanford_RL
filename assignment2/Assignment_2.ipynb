{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Test Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum sum of rewards achievable in the test environment given $\\gamma = 1$ should be 4.1. This value is achievable in a single trajectory, specifically: $s_{0} = 0$, $a_{0} = 2$, $R_{0} = 0$ $\\implies$ $s_{1} = 2$, $a_{1} = 1$, $R_{1} = 2$ $\\implies$ $s_{2} = 1$, $a_{2} = 2$, $R_{2} = 0$ $\\implies$ $s_{3} = 2$, $a_{3} = 1$, $R_{3} = 2$ $\\implies$ $s_{4} = 1$, $a_{4} = 0$, $R_{4} = 0.1$ $\\implies$ $s_{5} = 1$. We know from our table that the optimal move to make is the transition from state 2 to 1, giving a reward of +2. We also know that each episode can only contain a maximum of 5 actions and that the positive reward for other transitions is either 0.1 or 0.2. The previous constraint implies that no combination of non-optimal positive reward moves can ever amount to a cumulative reward greater or even equal to that of the optimal move. This conclusion entails that the strategy to maximize cumulative reward involves maximizing the amount of optimal moves we can perform with in a trajectory, and then filling in the leftover actions with non-optimal positive reward moves. Since it takes two actions to perform an optimal move, we can cram in 2 optimal moves per trajectory. The remaining action we perform a non-optimal positive reward action. The only non-optimal positive reward actions we have access to are those that give +0.1 reward. Therefore the maximum cumulative reward achievable in test environment is 2+2+0.1=4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we parameterized Q as a function of action as well we would have to exectute a forward pass for each legal action we can take from a given state inorder to calculate Q keeping action as a discrete parameter allows us to calculate Q in one forward pass instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight update given by equation: $\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w})  - \\hat{q}(s, a; \\vec{w})\\right)\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$ is not the result of stochastic gradient descent of objective: $L(\\vec{w}) = \\mathbb{E}_{s, a, r, s' \\sim D}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2}\\right]$\n",
    "\n",
    "The stochastic gradient descent update of an objective: $J(\\vec{w}) = \\mathbb{E}_{x \\sim D}\\left[l(x,\\vec{w})\\right]$ is given by: $\\vec{w} \\mapsto \\vec{w} - \\alpha \\nabla_{\\vec{w}} l(x,\\vec{w})$. This implies that the gradient descent weight update for $L(\\vec{w})$ should be given by: \n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} - \\nabla_{\\vec{w}}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2} \\right]$\n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} - 2 \\alpha \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q}(s, a; \\vec{w}) \\right)\\left(\\nabla_{\\vec{w}}\\left(\\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q} (s, a; \\vec{w})\\right)\\right)$\n",
    "\n",
    "$\\nabla_{\\vec{w}}\\left(\\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q} (s, a; \\vec{w})\\right) \\neq -\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$\n",
    "\n",
    "Therefore the update rule $\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w})  - \\hat{q}(s, a; \\vec{w})\\right)\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$ is not the result of stochastic gradient descent of objective: $L(\\vec{w}) = \\mathbb{E}_{s, a, r, s' \\sim D}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight update given by equation: $\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}})  - \\hat{q}(s, a; \\vec{w})\\right)\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$ is the result of stochastic gradient descent of objective: $L^{-}(\\vec{w}) = \\mathbb{E}_{s, a, r, s' \\sim D}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2}\\right]$\n",
    "\n",
    "The stochastic gradient descent update of an objective: $J(\\vec{w}) = \\mathbb{E}_{x \\sim D}\\left[l(x,\\vec{w})\\right]$ is given by: $\\vec{w} \\mapsto \\vec{w} - \\alpha \\nabla_{\\vec{w}} l(x,\\vec{w})$. This implies that the gradient descent weight update for $L(\\vec{w})$ should be given by: \n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} - \\nabla_{\\vec{w}}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2} \\right]$\n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} - 2 \\alpha \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q}(s, a; \\vec{w}) \\right)\\left(\\nabla_{\\vec{w}}\\left(\\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q} (s, a; \\vec{w})\\right)\\right)$\n",
    "\n",
    "$\\nabla_{\\vec{w}}\\left(\\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q} (s, a; \\vec{w})\\right) = -\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$\n",
    "\n",
    "Therefore the update rule $\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}})  - \\hat{q}(s, a; \\vec{w})\\right)\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$ is a result of stochastic gradient descent of the objective: $L^{-}(\\vec{w}) = \\mathbb{E}_{s, a, r, s' \\sim D}\\left[ \\left(r + \\gamma \\max\\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w^{-}}) - \\hat{q}(s, a; \\vec{w}) \\right)^{2}\\right]$ up to a factor of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more often we update our target networks weights, the better estimation we gain of the maximum Q value for a given state but the worse our learned functional representation of the Q function becomes, because our stochastic gradient descent update rule starts to break down. The longer it takes for us to update our target network weights, the worse estimation we gain of the maximum Q value for a given state, but the better our functional representation of Q becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning our target values our known, while in RL our target values are estimated. This implies that our distribution $D$ in the RL setup is constantly changing, while in the supervised learning setup, $D$ is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Linear Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $\\hat{q}(s, a; \\vec{w}) = \\vec{w} \\cdot \\vec{\\delta}(s - s_{\\tau}, a - a_{\\tau})$\n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w})  - \\hat{q}(s, a; \\vec{w})\\right)\\nabla_{\\vec{w}}\\hat{q}(s, a; \\vec{w})$\n",
    "\n",
    "$\\vec{w} \\mapsto \\vec{w} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}\\hat{q}(s', a'; \\vec{w})  - \\hat{q}(s, a; \\vec{w})\\right)\\vec{\\delta}(s-s_{\\tau}, a-a_{\\tau})$\n",
    "\n",
    "For terms where $s \\neq s_{\\tau}$, and $a \\neq a_{\\tau}$, $\\hat{q} = 0$ for all $\\vec{w}$. Therefore updates to $\\vec{w}$ only matter for when $s = s_{\\tau}$, and $a = a_{\\tau}$. Limiting $s$ and $a$ to only $s = s_{\\tau}$, and $a = a_{\\tau}$ terms yields:\n",
    "\n",
    "$w_{s_{\\tau} a_{\\tau}} \\mapsto w_{s_{\\tau} a_{\\tau}} + \\alpha\\left(r + \\gamma \\max \\limits_{a' \\in A}w_{s' a'} - w_{s_{\\tau} a_{\\tau}}\\right)$\n",
    "\n",
    "This equation is equivalent to the RL update rule for a $Q(s_{\\tau}, a_{\\tau})$ if we identify the components of $\\vec{w}$ with $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above question for proof of the new generalized update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do reach the optimal reward of 4.1. See plot of scores below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results/q2_linear/scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Implementing DeepMind's DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final performance when using a CNN as the function approximator, was the same as using a linear model. In both instances I reached the optimal reward value of 4.1. Although the DQN approach was slower when training than the linear model approach, and the variance in scores over each training epoch was much larger than it was for the linear function approximator. This is simply do to the fact that neural networks are a type of model with low bias and high variance as opposed to linear models which have high bias and low variance. I have attached the plot of scores vs training epochs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results/q3_nature/scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) DQN on Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance appeared to improve at first slowly, but over a larger amount of epochs, the total average reward appeared to decrease. Assuming we train for a larger amount of epochs will probably result with the score fluctuating about some number close to -21. Which is much worse than any human. This suggest that the linear function approximator does not contain enough variance to fully capture the Q-function for pong. A graph of cores vs epochs is attached below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results/q4_train_atari_linear/scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training both types of CNN architectures (Architecture 1: https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf, Architecture 2: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) for 5 million steps each, I found that the maximum score achievable on my current hardware+software setup to be about -3, which is human performance level. I am not sure why my scores have not achieved the expected range of 14-15. Some possible explanations could be that each of the 3 runs I did on each architecture happened to be unlucky. Or the non-deterministic GPU floating point calculations found in Nvidia's cuDNN library could be causing the non-reproducibility. It could also be an issue with my hardware, like my GPU's architecture. It could also be a bug in my code, although both CNN architectures were able to achieve the maximum scores on the test environment. The scores vs epoch graph of the best run out of the total of 6 runs is posted below (Architecture 1 performed the best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"results/q5_train_atari_nature_gen_res_so_far/scores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance gap between the DQN architecture and the linear Q value approximator can be explained by the fact that Deep Neural Networks tend to be high variance function approximators. This gives the DQN architecture an ability to learn more complex representations of the Q-value function when compared to the linear function approximator. The CNN style architecture of the DQN architecture, biases the function approximator to learn representations useful to computer vision like tasks, because convolution operations are good at learning local features. This further boosts the performance of the DQN architecture when compared to the linear Q value approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of DQN will not always improve monotonically over time. This is due to the fact that sub-optimal performance can always happen due to DQN's random exploration strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) n-step Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions:\n",
    "1) Current estimate of $Q$ for a given $s_{t} \\in \\mathbb{S}$ and $a_{t} \\in \\mathbb{A}$: $\\hat{q}(s_{t}, a_{t})$\n",
    "\n",
    "2) $Bias\\left(\\hat{q}(s_{t},a_{t})\\right) = \\left|Q^{\\pi}(s_{t}, a_{t}) - \\mathbb{E}\\left[\\hat{q}(s_{t},a_{t})|s_{t},a_{t}\\right]\\right| = \\beta$ for all $s_{t} \\in \\mathbb{S}$ and $a_{t} \\in \\mathbb{A}$\n",
    "\n",
    "3) $Q^{\\pi}(s_{t},a_{t}) = \\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{t+i}|s_{t},a_{t}\\right]$ where $r_{j} = R(s_{j},a_{j})$\n",
    "\n",
    "4) SARSA estimate of $Q$ for a given $s_{t} \\in \\mathbb{S}$ and $a_{t} \\in \\mathbb{A}$: $\\hat{q}_{s}(s_{t},a_{t}) = r_{t} + \\gamma \\hat{q}(s_{t+1},a_{t+1})$\n",
    "\n",
    "4) n-step SARSA estimate of $Q$ for a given $s_{t} \\in \\mathbb{S}$ and $a_{t} \\in \\mathbb{A}$: $\\hat{q}_{n}(s_{t},a_{t}) = \\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i} + \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})$\n",
    "\n",
    "5) $0 \\leq \\gamma < 1$, and $n \\geq 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove: $Bias\\left(\\hat{q}_{n}(s_{t}, a_{t})\\right) < Bias\\left(\\hat{q}_{s}(s_{t}, a_{t})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying $Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right)$\n",
    "\n",
    "\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\left|Q^{\\pi}(s_{t}, a_{t}) - \\mathbb{E}\\left[\\hat{q}_{s}(s_{t},a_{t})|s_{t},a_{t}\\right]\\right| = \\left|\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{t+i}|s_{t},a_{t}\\right] - \\mathbb{E}\\left[r_{t} + \\gamma \\hat{q}(s_{t+1},a_{t+1})|s_{t},a_{t}\\right]\\right|$\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\left|\\mathbb{E}\\left[\\sum_{i=1}^{\\infty}\\gamma^{i}r_{t+i} + r_{t} - r_{t} - \\gamma \\hat{q}(s_{t+1},a_{t+1})|s_{t},a_{t}\\right] \\right| = \\left|\\mathbb{E}\\left[\\sum_{i=1}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma \\hat{q}(s_{t+1},a_{t+1})|s_{t},a_{t}\\right] \\right|$\n",
    "\n",
    "Since $r_{j} = R(s_{j},a_{j})$\n",
    "\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\left|\\mathbb{E}\\left[\\sum_{i=1}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma \\hat{q}(s_{t+1},a_{t+1})|s_{t},a_{t}\\right] \\right| = \\left|\\mathbb{E}\\left[\\sum_{i=1}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma \\hat{q}(s_{t+1},a_{t+1})\\right] \\right|$\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\gamma \\left|\\mathbb{E}\\left[\\sum_{i=1}^{\\infty}\\gamma^{i-1}r_{t+i} - \\hat{q}(s_{t+1},a_{t+1})\\right] \\right| = \\gamma \\left|\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+1)+i} - \\hat{q}(s_{t+1},a_{t+1})\\right] \\right|$\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\gamma \\left|\\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+1)+i} - \\hat{q}(s_{t+1},a_{t+1})|s_{t+1},a_{t+1}\\right]\\right] \\right| = \\gamma \\left|\\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+1)+i}|s_{t+1},a_{t+1}\\right] - \\mathbb{E}\\left[\\hat{q}(s_{t+1},a_{t+1})|s_{t+1},a_{t+1}\\right]\\right] \\right|$\n",
    "$Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right) = \\gamma \\left|\\mathbb{E}\\left[Q^{\\pi}(s_{t+1},a_{t+1}) - \\mathbb{E}\\left[\\hat{q}(s_{t+1},a_{t+1})|s_{t+1},a_{t+1}\\right]\\right] \\right| = \\gamma \\left|\\mathbb{E}\\left[\\pm Bias\\left(\\hat{q}(s_{t+1},a_{t+1})\\right)\\right]\\right| = \\gamma \\left|\\mathbb{E}\\left[\\pm \\beta \\right]\\right| = \\gamma \\beta$\n",
    "\n",
    "\n",
    "Simplifying $Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right)$\n",
    "\n",
    "\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\left|Q^{\\pi}(s_{t}, a_{t}) - \\mathbb{E}\\left[\\hat{q}_{n}(s_{t},a_{t})|s_{t},a_{t}\\right]\\right| = \\left|\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{t+i}|s_{t},a_{t}\\right] - \\mathbb{E}\\left[\\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i} + \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})|s_{t},a_{t}\\right]\\right|$\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\left|\\mathbb{E}\\left[\\sum_{i=n}^{\\infty}\\gamma^{i}r_{t+i} + \\sum_{i=0}^{n-1} \\gamma^{i}r_{t+i} - \\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i} - \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})|s_{t},a_{t}\\right] \\right| = \\left|\\mathbb{E}\\left[\\sum_{i=n}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})|s_{t},a_{t}\\right] \\right|$\n",
    "\n",
    "Since $r_{j} = R(s_{j},a_{j})$\n",
    "\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\left|\\mathbb{E}\\left[\\sum_{i=n}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})|s_{t},a_{t}\\right] \\right| = \\left|\\mathbb{E}\\left[\\sum_{i=n}^{\\infty}\\gamma^{i}r_{t+i} - \\gamma^{n} \\hat{q}(s_{t+n},a_{t+n})\\right] \\right|$\n",
    "\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\gamma^{n} \\left|\\mathbb{E}\\left[\\sum_{i=n}^{\\infty}\\gamma^{i-n}r_{t+i} - \\hat{q}(s_{t+n},a_{t+n})\\right] \\right| = \\gamma^{n} \\left|\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+n)+i} - \\hat{q}(s_{t+n},a_{t+n})\\right] \\right|$\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\gamma^{n} \\left|\\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+n)+i} - \\hat{q}(s_{t+n},a_{t+n})|s_{t+n},a_{t+n}\\right]\\right]\\right| = \\gamma^{n} \\left|\\mathbb{E}\\left[\\mathbb{E}\\left[\\sum_{i=0}^{\\infty}\\gamma^{i}r_{(t+n)+i} | s_{t+n},a_{t+n}\\right] - \\mathbb{E}\\left[ \\hat{q}(s_{t+n},a_{t+n})|s_{t+n},a_{t+n}\\right]\\right]\\right|$\n",
    "$Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) = \\gamma^{n} \\left|\\mathbb{E}\\left[Q^{\\pi}(s_{t+n},a_{t+n}) - \\mathbb{E}\\left[ \\hat{q}(s_{t+n},a_{t+n})|s_{t+n},a_{t+n}\\right]\\right]\\right| = \\gamma^{n} \\left| \\mathbb{E}\\left[ \\pm Bias\\left(\\hat{q}(s_{t+n},a_{t+n})\\right) \\right] \\right| = \\gamma^{n} \\left|\\mathbb{E}\\left[ \\pm \\beta \\right]\\right| = \\gamma^{n}\\beta$\n",
    "\n",
    "Since $0 \\leq \\gamma < 1$, and $n \\geq 2$ $$\\gamma^{n}\\beta < \\gamma\\beta \\implies Bias\\left(\\hat{q}_{n}(s_{t},a_{t})\\right) < Bias\\left(\\hat{q}_{s}(s_{t},a_{t})\\right)$$ QED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
